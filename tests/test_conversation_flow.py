"""Conversation flow tests â€” 10x scale (7000+ tests).

Validates that the agent loop produces natural, well-structured conversations:
  - Message ordering: system â†’ user â†’ assistant â†’ user â†’ assistant â†’ ...
  - Nudge messages are injected at the correct position
  - Tool results appear as user messages after assistant tool calls
  - No corrupted or out-of-order messages
  - Conversation content is meaningful (not empty, not garbled)
  - Natural teaching patterns: explain â†’ write â†’ predict â†’ run â†’ choose

Simulates 1000 users Ã— 10 turns for run_turn and run_turn_stream.
"""

from __future__ import annotations

import itertools
import random
import re
import string
from unittest.mock import MagicMock

import pytest

from novicode.agent_loop import (
    AgentLoop,
    _has_code_block,
    _MAX_NUDGES_PER_TURN,
    _TOOL_NUDGE,
)
from novicode.config import Mode, LanguageFamily, build_mode_profile, MODE_LANGUAGE
from novicode.curriculum import Level, CONCEPT_CATALOGS, build_education_prompt
from novicode.llm_adapter import LLMResponse, Message, ToolCall, TOOL_DEFINITIONS
from novicode.metrics import Metrics
from novicode.policy_engine import PolicyEngine
from novicode.session_manager import Session, SessionMeta
from novicode.validator import ValidationResult, Violation


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _session():
    return Session(meta=SessionMeta(
        session_id="flow", model="qwen3:8b", mode="python_basic",
        created_at=0.0, research=False,
    ))


def _loop(llm, mode=Mode.PYTHON_BASIC, max_iter=20):
    profile = build_mode_profile(mode)
    policy = PolicyEngine(profile, level=Level.BEGINNER)
    tools = MagicMock()
    tools.available_tools.return_value = list(profile.allowed_tools)
    tools.execute.return_value = {"status": "ok"}
    validator = MagicMock()
    validator.validate.return_value = ValidationResult(valid=True)
    return AgentLoop(
        llm=llm, profile=profile, tools=tools, validator=validator,
        policy=policy, session=_session(), metrics=Metrics(),
        max_iterations=max_iter,
    )


def _resp(content="", tool_calls=None):
    return LLMResponse(content=content, tool_calls=tool_calls or [])


def _tc(name="write", **kwargs):
    if not kwargs:
        kwargs = {"path": "test.py", "content": "x = 1"}
    return ToolCall(name=name, arguments=kwargs)


# â”€â”€ Conversation validators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def assert_message_ordering(messages: list[Message]) -> None:
    """Verify that messages follow valid role ordering rules."""
    assert len(messages) >= 1, "Must have at least system message"
    assert messages[0].role == "system", f"First message must be system, got {messages[0].role}"

    for i in range(1, len(messages)):
        prev = messages[i - 1]
        curr = messages[i]

        # After system: must be user
        if i == 1:
            assert curr.role == "user", (
                f"Message[1] must be user after system, got {curr.role}"
            )

        # After assistant: must be user (could be user input, nudge, tool result, or correction)
        if prev.role == "assistant":
            assert curr.role == "user", (
                f"Message[{i}] after assistant must be user, got {curr.role}: "
                f"{curr.content[:60]!r}"
            )

        # After user (that isn't the first): must be assistant
        # EXCEPT: consecutive user messages can happen if system prompt was replaced
        # But generally: user â†’ assistant
        if prev.role == "user" and i > 1:
            assert curr.role in ("assistant", "system"), (
                f"Message[{i}] after user should be assistant, got {curr.role}: "
                f"{curr.content[:60]!r}"
            )


def assert_no_empty_assistant(messages: list[Message]) -> None:
    """Warn if assistant messages are empty (might indicate LLM issues)."""
    for i, m in enumerate(messages):
        if m.role == "assistant":
            # Empty content is ok if there were tool calls (content can be "")
            # But we just check it doesn't cause structural issues
            assert isinstance(m.content, str), f"Message[{i}] content must be str"


def assert_nudge_placement(messages: list[Message]) -> None:
    """Verify nudge messages are correctly placed: always as user after assistant."""
    for i, m in enumerate(messages):
        if m.content == _TOOL_NUDGE:
            assert m.role == "user", f"Nudge at [{i}] must be user role"
            assert i > 0, "Nudge can't be first message"
            assert messages[i - 1].role == "assistant", (
                f"Nudge at [{i}] must follow assistant, "
                f"but follows {messages[i-1].role}"
            )


def assert_tool_results_placement(messages: list[Message]) -> None:
    """Verify tool result messages follow assistant messages."""
    for i, m in enumerate(messages):
        if m.role == "user" and "Tool results:" in m.content:
            assert i > 0
            assert messages[i - 1].role == "assistant", (
                f"Tool results at [{i}] must follow assistant"
            )


def assert_system_prompt_integrity(messages: list[Message]) -> None:
    """System prompt must remain at index 0 and contain key content."""
    assert messages[0].role == "system"
    prompt = messages[0].content
    assert len(prompt) > 50, "System prompt is too short"
    # Must contain educational content or base prompt
    assert any(kw in prompt for kw in ["åˆ¶ç´„", "tutor", "å…ˆç”Ÿ"]), (
        "System prompt missing expected content"
    )


def validate_conversation(messages: list[Message]) -> None:
    """Run all conversation validators."""
    assert_message_ordering(messages)
    assert_no_empty_assistant(messages)
    assert_nudge_placement(messages)
    assert_tool_results_placement(messages)
    assert_system_prompt_integrity(messages)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Natural teaching flow patterns â€” parameterized
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Simulate realistic teaching flows:
# Pattern A: User asks â†’ LLM writes code with tool â†’ asks prediction
# Pattern B: User asks â†’ LLM outputs code as text (nudge) â†’ retries with tool
# Pattern C: User predicts â†’ LLM runs code with bash â†’ gives feedback
# Pattern D: User asks question â†’ LLM explains (text only)
# Pattern E: User greets â†’ LLM starts with explanation + code
# Pattern F: Validation failure â†’ correction â†’ success

_TEACHING_FLOWS = {
    "write_and_predict": {
        "user_input": "å¤‰æ•°ã«ã¤ã„ã¦æ•™ãˆã¦",
        "responses": [
            _resp(
                "å¤‰æ•°ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã­ã€‚ã¾ãšç°¡å˜ãªã‚³ãƒ¼ãƒ‰ã‚’ä½œã‚Šã¾ã™ã€‚",
                [_tc("write", path="hello.py", content="x = 10\nprint(x)")],
            ),
            _resp(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
                "- `x = 10` ã¯å¤‰æ•° x ã« 10 ã‚’ä»£å…¥ã—ã¦ã„ã¾ã™\n"
                "- `print(x)` ã¯ x ã®å€¤ã‚’è¡¨ç¤ºã—ã¾ã™\n\n"
                "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_tool_call": True,
            "final_has_question": True,
        },
    },
    "run_and_feedback": {
        "user_input": "10ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¨æ€ã„ã¾ã™",
        "responses": [
            _resp(
                "å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                [_tc("bash", command="python hello.py")],
            ),
            _resp(
                "æ­£è§£ã§ã™ï¼ `10` ã¨è¡¨ç¤ºã•ã‚Œã¾ã—ãŸã­ã€‚\n\n"
                "æ¬¡ã¯ A. å¤‰æ•°ã‚’2ã¤ä½¿ã£ã¦è¶³ã—ç®—ã™ã‚‹ ã‹ "
                "B. æ–‡å­—åˆ—ã®å¤‰æ•°ã‚’ä½œã‚‹ ã®ã©ã¡ã‚‰ã«ã—ã¾ã—ã‚‡ã†ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_tool_call": True,
            "final_has_choices": True,
        },
    },
    "text_explanation": {
        "user_input": "å¤‰æ•°ã£ã¦ä½•ï¼Ÿ",
        "responses": [
            _resp(
                "å¤‰æ•°ã¨ã¯ã€å€¤ã‚’å…¥ã‚Œã¦ãŠãç®±ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã€‚\n"
                "åå‰ã‚’ã¤ã‘ã¦ã€å¾Œã‹ã‚‰å€¤ã‚’å–ã‚Šå‡ºã›ã¾ã™ã€‚\n\n"
                "å¤‰æ•°ã«ã¤ã„ã¦ã¯åˆ†ã‹ã‚Šã¾ã™ã‹ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_tool_call": False,
            "final_is_text": True,
        },
    },
    "nudge_then_tool": {
        "user_input": "forãƒ«ãƒ¼ãƒ—ã‚’æ•™ãˆã¦",
        "responses": [
            _resp("ã“ã¡ã‚‰ãŒã‚³ãƒ¼ãƒ‰ã§ã™ï¼š\n```python\nfor i in range(5):\n    print(i)\n```\n"),
            _resp(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚",
                [_tc("write", path="loop.py", content="for i in range(5):\n    print(i)")],
            ),
            _resp(
                "ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
                "- `for i in range(5):` ã¯ 0ã€œ4 ã®5å›ç¹°ã‚Šè¿”ã—ã¾ã™\n"
                "- `print(i)` ã¯ i ã®å€¤ã‚’è¡¨ç¤ºã—ã¾ã™\n\n"
                "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_nudge": True,
            "has_tool_call": True,
        },
    },
    "greeting_start": {
        "user_input": "ã“ã‚“ã«ã¡ã¯",
        "responses": [
            _resp(
                "ã“ã‚“ã«ã¡ã¯ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’ä¸€ç·’ã«å­¦ã³ã¾ã—ã‚‡ã†ã€‚\n"
                "ã¾ãšã¯ç°¡å˜ãªã¨ã“ã‚ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚",
                [_tc("write", path="hello.py", content="print('Hello!')")],
            ),
            _resp(
                "æœ€åˆã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚Šã¾ã—ãŸã€‚\n\n"
                "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_tool_call": True,
        },
    },
    "empty_user_input": {
        "user_input": "",
        "responses": [
            _resp("ä½•ã‹è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã©ã†ãï¼"),
        ],
        "checks": {
            "has_tool_call": False,
        },
    },
    "user_says_ok": {
        "user_input": "OK",
        "responses": [
            _resp(
                "ã§ã¯æ¬¡ã«é€²ã¿ã¾ã—ã‚‡ã†ã€‚",
                [_tc("write", path="step2.py", content="y = 20\nprint(y)")],
            ),
            _resp(
                "æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n\n"
                "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
            ),
        ],
        "checks": {
            "has_tool_call": True,
        },
    },
    "user_says_next": {
        "user_input": "æ¬¡",
        "responses": [
            _resp("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚ãƒªã‚¹ãƒˆã‚’å­¦ã³ã¾ã—ã‚‡ã†ã€‚"),
        ],
        "checks": {
            "has_tool_call": False,
        },
    },
    "validation_failure_then_success": {
        "user_input": "HTMLã‚’æ›¸ã„ã¦",
        "responses": [
            _resp("<html><body>Hello</body></html>"),  # violation in python mode
            _resp("ã™ã¿ã¾ã›ã‚“ã€Python ãƒ¢ãƒ¼ãƒ‰ãªã®ã§ Python ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã¾ã™ã­ã€‚\nx = 1"),
        ],
        "checks": {
            "has_violation": True,
        },
    },
    "multiple_tool_calls": {
        "user_input": "2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã£ã¦",
        "responses": [
            _resp("2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã‚Šã¾ã™ã€‚", [
                _tc("write", path="a.py", content="a = 1"),
                _tc("write", path="b.py", content="b = 2"),
            ]),
            _resp("2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚"),
        ],
        "checks": {
            "tool_count": 2,
        },
    },
}


class TestNaturalTeachingFlows:

    @pytest.mark.parametrize("flow_name,flow", _TEACHING_FLOWS.items(),
                             ids=list(_TEACHING_FLOWS.keys()))
    def test_flow_produces_valid_conversation(self, flow_name, flow):
        """Each teaching flow should produce a well-structured conversation."""
        llm = MagicMock()
        llm.chat.side_effect = list(flow["responses"])
        loop = _loop(llm)

        # Handle validation failure flow
        if flow.get("checks", {}).get("has_violation"):
            loop.validator.validate.side_effect = [
                ValidationResult(valid=False, violations=[
                    Violation(rule="language_isolation", detail="HTML in Python")
                ]),
                ValidationResult(valid=True),
            ]

        result = loop.run_turn(flow["user_input"])
        validate_conversation(loop.messages)

        checks = flow.get("checks", {})
        if checks.get("has_tool_call"):
            assert loop.tools.execute.called, f"{flow_name}: expected tool call"
        if checks.get("has_nudge"):
            nudges = [m for m in loop.messages if m.content == _TOOL_NUDGE]
            assert len(nudges) >= 1, f"{flow_name}: expected nudge"
        if checks.get("final_has_question"):
            assert "æ€ã„ã¾ã™ã‹" in result, f"{flow_name}: expected prediction question"
        if checks.get("final_has_choices"):
            assert any(c in result for c in ["A.", "B.", "ã©ã¡ã‚‰"]), (
                f"{flow_name}: expected choices"
            )
        if checks.get("tool_count"):
            assert loop.tools.execute.call_count == checks["tool_count"]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Message ordering â€” 1000 users Ã— 10 turns (run_turn)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# User input variety
_USER_INPUTS_JA = [
    "ã“ã‚“ã«ã¡ã¯", "å¤‰æ•°ã«ã¤ã„ã¦æ•™ãˆã¦", "forãƒ«ãƒ¼ãƒ—ã®æ›¸ãæ–¹", "é–¢æ•°ã£ã¦ä½•ï¼Ÿ",
    "ãƒªã‚¹ãƒˆã®ä½¿ã„æ–¹", "ã‚¯ãƒ©ã‚¹ã‚’æ•™ãˆã¦", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã£ã¦", "ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦",
    "ã‚‚ã†ä¸€åº¦", "æ¬¡", "ç¶šã", "OK", "åˆ†ã‹ã‚Šã¾ã—ãŸ", "åˆ†ã‹ã‚‰ãªã„",
    "10ãŒå‡ºã‚‹ã¨æ€ã†", "ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã¨æ€ã†", "ä½•ã‚‚å‡ºãªã„ã¨æ€ã†",
    "Aã‚’é¸ã³ã¾ã™", "Bã«ã—ã¾ã™", "ä¸¡æ–¹ã‚„ã‚ŠãŸã„",
    "printé–¢æ•°", "ifæ–‡", "while", "defã§é–¢æ•°",
    "ã‚ã‚ŠãŒã¨ã†", "ã™ã”ã„", "é¢ç™½ã„", "é›£ã—ã„",
    "ã‚‚ã£ã¨ç°¡å˜ã«ã—ã¦", "ã‚‚ã£ã¨è©³ã—ã", "ä¾‹ã‚’è¦‹ã›ã¦",
]

_USER_INPUTS_EN = [
    "Hello", "Teach me variables", "How does a for loop work?",
    "What is a function?", "Show me lists", "Write a class",
    "Create a file", "Run the code", "Again", "Next", "OK",
    "I think it prints 10", "I don't understand", "Option A",
]

_ALL_USER_INPUTS = _USER_INPUTS_JA + _USER_INPUTS_EN

# LLM response patterns (simulating realistic Qwen3 outputs)
_LLM_PATTERNS = {
    "explain_and_write": lambda rng: [
        _resp(
            f"{'èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ' * rng.randint(1, 3)}",
            [_tc("write", path=f"step{rng.randint(1,99)}.py",
                 content=f"x = {rng.randint(1,100)}\nprint(x)")],
        ),
        _resp(
            "ä¿å­˜ã—ã¾ã—ãŸã€‚\n\nã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
        ),
    ],
    "run_code": lambda rng: [
        _resp(
            "å®Ÿè¡Œã—ã¾ã™ã€‚",
            [_tc("bash", command=f"python step{rng.randint(1,99)}.py")],
        ),
        _resp(
            f"çµæœã¯ {rng.randint(1,100)} ã§ã—ãŸã€‚\n\n"
            "æ¬¡ã¯ A ã‹ B ã®ã©ã¡ã‚‰ã«ã—ã¾ã—ã‚‡ã†ï¼Ÿ"
        ),
    ],
    "text_only": lambda rng: [
        _resp(rng.choice([
            "å¤‰æ•°ã¨ã¯å€¤ã‚’å…¥ã‚Œã‚‹ç®±ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã€‚",
            "forãƒ«ãƒ¼ãƒ—ã¯ç¹°ã‚Šè¿”ã—å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚",
            "é–¢æ•°ã‚’ä½¿ã†ã¨ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨ã§ãã¾ã™ã€‚",
            "ãƒªã‚¹ãƒˆã¯è¤‡æ•°ã®å€¤ã‚’ã¾ã¨ã‚ã¦æ‰±ãˆã¾ã™ã€‚",
            "ã‚¯ãƒ©ã‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã¨å‡¦ç†ã‚’ã¾ã¨ã‚ã‚‹ã‚‚ã®ã§ã™ã€‚",
            "åˆ†ã‹ã‚Šã¾ã—ãŸã€‚ã‚‚ã†å°‘ã—ç°¡å˜ã«èª¬æ˜ã—ã¾ã™ã­ã€‚",
            "ã„ã„è³ªå•ã§ã™ã­ï¼",
            "ç´ æ™´ã‚‰ã—ã„äºˆæ¸¬ã§ã™ï¼æ­£è§£ã§ã™ã€‚",
        ])),
    ],
    "code_block_nudge": lambda rng: [
        _resp(f"```python\nx = {rng.randint(1,100)}\nprint(x)\n```\n"),
        _resp("", [_tc("write", path="f.py", content="x=1\nprint(x)")]),
        _resp("ä¿å­˜ã—ã¾ã—ãŸã€‚ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"),
    ],
    "empty_then_text": lambda rng: [
        _resp(""),  # empty first
        # Note: empty response goes to validation which is mocked as valid
    ],
    "multi_tool": lambda rng: [
        _resp("è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã‚Šã¾ã™ã€‚", [
            _tc("write", path="a.py", content="a=1"),
            _tc("write", path="b.py", content="b=2"),
        ]),
        _resp("å®Œäº†ã—ã¾ã—ãŸã€‚"),
    ],
    "read_then_edit": lambda rng: [
        _resp("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚", [_tc("read", path="step1.py")]),
        _resp("ç·¨é›†ã—ã¾ã™ã€‚", [
            _tc("edit", path="step1.py", old_string="x=1", new_string="x=2")
        ]),
        _resp("ä¿®æ­£ã—ã¾ã—ãŸã€‚"),
    ],
}


class TestMassiveRunTurn:
    """1000 users Ã— 10 turns with full conversation flow validation."""

    @pytest.mark.parametrize("user_id", range(1000))
    def test_user_session(self, user_id):
        rng = random.Random(user_id * 31337)
        mode = rng.choice(list(Mode))
        llm = MagicMock()
        loop = _loop(llm, mode=mode, max_iter=15)

        for turn in range(10):
            pattern_name = rng.choice(list(_LLM_PATTERNS.keys()))
            responses = _LLM_PATTERNS[pattern_name](rng)
            llm.chat.side_effect = list(responses)

            user_input = rng.choice(_ALL_USER_INPUTS)

            try:
                result = loop.run_turn(user_input)
                assert isinstance(result, str), (
                    f"User {user_id} turn {turn}: result not str"
                )
                # Validate conversation structure
                validate_conversation(loop.messages)

            except StopIteration:
                # side_effect exhausted â€” acceptable in random testing
                pass

            # Reset messages but keep system prompt for next turn
            loop.messages = [loop.messages[0]]
            loop.validator.validate.return_value = ValidationResult(valid=True)
            loop.validator.validate.side_effect = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Message ordering â€” 1000 users Ã— 10 turns (run_turn_stream)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _make_stream(pattern_name, rng):
    """Create a stream generator function for a given pattern."""
    if pattern_name == "text_stream":
        text = rng.choice([
            "å¤‰æ•°ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã­ã€‚",
            "forãƒ«ãƒ¼ãƒ—ã®ä½¿ã„æ–¹ã§ã™ã€‚",
            "ä¿å­˜ã—ã¾ã—ãŸã€‚ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ",
            "æ­£è§£ã§ã™ï¼ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ã€‚",
            "æ¬¡ã¯ä½•ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
        ])
        def _s(messages, tools=None):
            # Yield character by character (simulating streaming)
            for ch in text[:20]:  # first 20 chars
                yield ch
            yield LLMResponse(content=text, tool_calls=[])
        return _s

    elif pattern_name == "tool_stream":
        def _s(messages, tools=None):
            yield LLMResponse(
                content="ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã€‚",
                tool_calls=[_tc("write", path="f.py", content="x=1")],
            )
        return _s

    elif pattern_name == "code_block_stream":
        code = f"```python\nx = {rng.randint(1,100)}\n```\n"
        def _s(messages, tools=None):
            yield code
            yield LLMResponse(content=code, tool_calls=[])
        return _s

    elif pattern_name == "empty_stream":
        def _s(messages, tools=None):
            yield LLMResponse(content="", tool_calls=[])
        return _s

    else:  # fallback text
        def _s(messages, tools=None):
            yield "OK"
            yield LLMResponse(content="OK", tool_calls=[])
        return _s


class TestMassiveRunTurnStream:
    """1000 users Ã— 10 turns streaming with conversation validation."""

    @pytest.mark.parametrize("user_id", range(1000))
    def test_stream_user_session(self, user_id):
        rng = random.Random(user_id * 65537)
        mode = rng.choice(list(Mode))
        llm = MagicMock()
        loop = _loop(llm, mode=mode, max_iter=10)

        stream_patterns = ["text_stream", "tool_stream", "code_block_stream",
                           "empty_stream"]

        for turn in range(10):
            pattern = rng.choice(stream_patterns)

            # Provide enough streams for nudge retries
            streams = [_make_stream(pattern, rng)]
            for _ in range(5):
                streams.append(_make_stream("text_stream", rng))
            llm.chat_stream.side_effect = streams

            user_input = rng.choice(_ALL_USER_INPUTS)

            try:
                chunks = list(loop.run_turn_stream(user_input))
                for chunk in chunks:
                    assert isinstance(chunk, str), (
                        f"User {user_id} turn {turn}: chunk not str: {type(chunk)}"
                    )
                # Validate conversation
                validate_conversation(loop.messages)

            except (StopIteration, TypeError):
                pass

            loop.messages = [loop.messages[0]]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Conversation content quality checks
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestConversationContentQuality:
    """Verify the content of generated conversations is sensible."""

    @pytest.mark.parametrize("mode", list(Mode))
    def test_system_prompt_matches_mode(self, mode):
        """System prompt should reference the correct domain."""
        profile = build_mode_profile(mode)
        policy = PolicyEngine(profile, level=Level.BEGINNER)
        prompt = policy.build_system_prompt()

        domain_keywords = {
            Mode.PYTHON_BASIC: ["Python"],
            Mode.PY5: ["Py5", "Processing"],
            Mode.SKLEARN: ["scikit-learn", "æ©Ÿæ¢°å­¦ç¿’"],
            Mode.PANDAS: ["pandas", "ãƒ‡ãƒ¼ã‚¿åˆ†æ"],
            Mode.WEB_BASIC: ["HTML", "CSS", "JavaScript", "Web"],
            Mode.AFRAME: ["A-Frame", "WebXR", "3D"],
            Mode.THREEJS: ["Three.js", "3D"],
        }
        keywords = domain_keywords[mode]
        assert any(kw in prompt for kw in keywords), (
            f"Mode {mode.value} prompt should mention {keywords}"
        )

    @pytest.mark.parametrize("mode", list(Mode))
    def test_prediction_question_in_teaching_flow(self, mode):
        """Teaching flow should end with prediction question."""
        llm = MagicMock()
        llm.chat.side_effect = [
            _resp("ã‚³ãƒ¼ãƒ‰ã‚’ä½œã‚Šã¾ã™ã€‚", [
                _tc("write", path="test.py", content="print(42)")
            ]),
            _resp(
                "ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
                "ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã©ã‚“ãªçµæœã«ãªã‚‹ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ"
            ),
        ]
        loop = _loop(llm, mode=mode)
        result = loop.run_turn("æ•™ãˆã¦")
        assert "æ€ã„ã¾ã™ã‹" in result

    @pytest.mark.parametrize("mode", list(Mode))
    def test_tool_usage_logged_in_metrics(self, mode):
        """Tool calls should be recorded in metrics."""
        llm = MagicMock()
        llm.chat.side_effect = [
            _resp("Saving...", [_tc("write", path="a.py", content="x=1")]),
            _resp("Done!"),
        ]
        loop = _loop(llm, mode=mode)
        loop.run_turn("Write code")
        assert loop.metrics.tool_calls.get("write", 0) >= 1

    def test_nudge_message_is_natural_japanese(self):
        """Nudge message should read naturally in Japanese."""
        assert "ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯" in _TOOL_NUDGE
        assert "write" in _TOOL_NUDGE
        # Should be a complete sentence, not a fragment
        assert _TOOL_NUDGE.endswith("ã€‚") or _TOOL_NUDGE.endswith("ãã ã•ã„ã€‚")
        # Not too long
        assert len(_TOOL_NUDGE) < 200

    def test_nudge_does_not_expose_internal_terms(self):
        """Nudge should not use overly technical internal terms."""
        # "ãƒ„ãƒ¼ãƒ«" is ok (used in curriculum), but avoid implementation details
        assert "LLMResponse" not in _TOOL_NUDGE
        assert "_has_code_block" not in _TOOL_NUDGE
        assert "side_effect" not in _TOOL_NUDGE

    @pytest.mark.parametrize("mode,level", list(itertools.product(Mode, Level)))
    def test_education_prompt_is_natural_japanese(self, mode, level):
        """Education prompts should be well-formed Japanese."""
        prompt = build_education_prompt(mode, level)
        # Should contain Japanese text
        has_ja = any(
            '\u3040' <= c <= '\u9fff' or '\u30a0' <= c <= '\u30ff'
            for c in prompt
        )
        assert has_ja, f"Prompt for {mode}/{level} should contain Japanese"
        # Should not have unresolved template variables
        assert "{" not in prompt or "{{" in prompt or all(
            v in prompt for v in []
        ), f"Unresolved template in {mode}/{level}: {prompt[:100]}"

    @pytest.mark.parametrize("mode", list(Mode))
    def test_constraint_section_present(self, mode):
        """Constraint section should always be present."""
        profile = build_mode_profile(mode)
        policy = PolicyEngine(profile)
        prompt = policy.build_system_prompt()
        assert "ã€åˆ¶ç´„ã€‘" in prompt
        assert "æœ€å¤§10è¡Œ" in prompt
        assert "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡" in prompt


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. Edge case combos at scale
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestEdgeCaseCombos:

    @pytest.mark.parametrize("n_nudges", range(_MAX_NUDGES_PER_TURN + 3))
    def test_various_nudge_counts(self, n_nudges):
        """Test with 0 to _MAX_NUDGES+2 code block responses before clean."""
        llm = MagicMock()
        code = "```python\nprint('hi')\n```\n"
        responses = [_resp(code)] * n_nudges + [_resp("Clean text.")]
        llm.chat.side_effect = responses
        loop = _loop(llm, max_iter=n_nudges + 5)

        result = loop.run_turn("Hello")
        validate_conversation(loop.messages)

        nudges = [m for m in loop.messages if m.content == _TOOL_NUDGE]
        expected_nudges = min(n_nudges, _MAX_NUDGES_PER_TURN)
        assert len(nudges) == expected_nudges

    @pytest.mark.parametrize("n_violations", range(1, 6))
    def test_various_violation_counts(self, n_violations):
        """Test with 1 to 5 validation failures before success."""
        llm = MagicMock()
        responses = (
            [_resp("bad response")] * n_violations
            + [_resp("good response")]
        )
        llm.chat.side_effect = responses
        loop = _loop(llm, max_iter=n_violations + 5)

        violations = [
            ValidationResult(valid=False, violations=[
                Violation(rule="language_isolation", detail="test")
            ])
        ] * n_violations + [ValidationResult(valid=True)]
        loop.validator.validate.side_effect = violations

        result = loop.run_turn("Hello")
        validate_conversation(loop.messages)
        assert loop.metrics.violations == n_violations

    @pytest.mark.parametrize("combo", [
        # (nudges_before, violations_after, tool_at_end)
        (1, 0, False),
        (2, 0, False),
        (0, 1, False),
        (0, 2, False),
        (1, 1, False),
        (2, 1, False),
        (1, 0, True),
        (0, 0, True),
        (2, 0, True),
        (1, 1, True),
    ])
    def test_nudge_violation_tool_combos(self, combo):
        """Test combinations of nudges, violations, and tool calls."""
        n_nudges, n_violations, tool_at_end = combo
        llm = MagicMock()

        responses = []
        # Code blocks for nudges
        for _ in range(n_nudges):
            responses.append(_resp("```python\ncode\n```\n"))
        # Bad responses for violations
        for _ in range(n_violations):
            responses.append(_resp("bad"))
        # Final response
        if tool_at_end:
            responses.append(_resp("tool", [_tc("write", path="f.py", content="x=1")]))
            responses.append(_resp("Done!"))
        else:
            responses.append(_resp("Good text."))

        llm.chat.side_effect = responses
        loop = _loop(llm, max_iter=20)

        # Set up validation side effects
        validation_results = []
        # Nudged responses don't go to validation
        # After nudges exhausted, code block goes to validation
        # Then violation responses
        for _ in range(n_violations):
            validation_results.append(ValidationResult(
                valid=False,
                violations=[Violation(rule="test", detail="test")]
            ))
        validation_results.append(ValidationResult(valid=True))
        # Add extras for safety
        for _ in range(10):
            validation_results.append(ValidationResult(valid=True))
        loop.validator.validate.side_effect = validation_results

        result = loop.run_turn("Hello")
        validate_conversation(loop.messages)

    @pytest.mark.parametrize("content", [
        "\n" * 100,  # just newlines
        " " * 100,  # just spaces
        "\t" * 50,  # just tabs
        "a",  # single char
        "ã‚",  # single Japanese char
        "ğŸ‰",  # emoji
        "ğŸ‰" * 100,  # many emojis
        "\x00",  # null byte
        "\\n\\t\\r",  # escaped chars as literals
        "```",  # just triple backtick, no newline
        "`" * 100,  # many backticks
        "---\n---\n---",  # markdown HR
        "| a | b |\n|---|---|\n| 1 | 2 |",  # markdown table
        "$$x^2$$",  # LaTeX
        "<script>alert(1)</script>",  # XSS attempt
        "'; DROP TABLE users; --",  # SQL injection attempt
    ])
    def test_weird_content_no_crash(self, content):
        """Agent loop should handle any content without crashing."""
        llm = MagicMock()
        llm.chat.return_value = _resp(content)
        loop = _loop(llm)
        result = loop.run_turn("Hello")
        assert isinstance(result, str)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. Code block detection at 10x scale
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Generate 500 random code-block-containing strings
_RANDOM_CODE_BLOCKS = []
for i in range(500):
    rng = random.Random(i * 999)
    lang = rng.choice(["python", "js", "html", "css", "bash", "go", "rust",
                        "java", "ruby", "php", "sql", "yaml", "json", ""])
    prefix = rng.choice(["", "text before\n", "èª¬æ˜ï¼š\n\n", "# Title\n\n"])
    suffix = rng.choice(["", "\ntext after", "\n\næ¬¡ã¸", "\n\nè³ªå•ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"])
    code_lines = rng.randint(1, 20)
    code = "\n".join(f"line{j} = {j}" for j in range(code_lines))
    text = f"{prefix}```{lang}\n{code}\n```{suffix}"
    _RANDOM_CODE_BLOCKS.append(text)

# Generate 500 random non-code-block strings
_RANDOM_NON_CODE = []
for i in range(500):
    rng = random.Random(i * 777)
    pattern = rng.choice([
        lambda r: "".join(r.choices("ã‚ã„ã†ãˆãŠã‹ããã‘ã“", k=r.randint(10, 200))),
        lambda r: "".join(r.choices(string.ascii_letters + " \n", k=r.randint(10, 200))),
        lambda r: f"Step {r.randint(1,10)}: {r.choice(['å¤‰æ•°', 'é–¢æ•°', 'ãƒ«ãƒ¼ãƒ—'])}ã‚’å­¦ã¶",
        lambda r: f"`code` and `more code` but not a block",
        lambda r: "- item\n" * r.randint(1, 10),
        lambda r: "# " + "".join(r.choices("ABC", k=5)) + "\n\nText.",
        lambda r: "",
        lambda r: " ",
    ])
    _RANDOM_NON_CODE.append(pattern(rng))


class TestCodeBlockDetection10x:

    @pytest.mark.parametrize("text", _RANDOM_CODE_BLOCKS[:500],
                             ids=[f"has_code_{i}" for i in range(500)])
    def test_detects_code_block(self, text):
        assert _has_code_block(text) is True

    @pytest.mark.parametrize("text", _RANDOM_NON_CODE[:500],
                             ids=[f"no_code_{i}" for i in range(500)])
    def test_no_false_positive(self, text):
        assert _has_code_block(text) is False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. All mode Ã— all tool combinations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_ALL_TOOLS = ["bash", "read", "write", "edit", "grep", "glob"]


class TestAllModeToolCombos:

    @pytest.mark.parametrize("mode,tool_name",
                             [(m, t) for m in Mode for t in _ALL_TOOLS],
                             ids=[f"{m.value}_{t}" for m in Mode for t in _ALL_TOOLS])
    def test_tool_allowed_consistency(self, mode, tool_name):
        """Tool allowed/blocked should be consistent with mode language."""
        profile = build_mode_profile(mode)
        policy = PolicyEngine(profile)
        verdict = policy.check_tool_allowed(tool_name)

        lang = MODE_LANGUAGE[mode]
        if tool_name == "bash":
            if lang == LanguageFamily.PYTHON:
                assert verdict.allowed, f"bash should be allowed in {mode.value}"
            else:
                assert not verdict.allowed, f"bash should be blocked in {mode.value}"
        else:
            # read, write, edit, grep, glob â€” allowed in all modes
            assert verdict.allowed, f"{tool_name} should be allowed in {mode.value}"
